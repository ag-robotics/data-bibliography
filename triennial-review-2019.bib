
@article{dias_multispecies_2018,
	title = {Multispecies {Fruit} {Flower} {Detection} {Using} a {Refined} {Semantic} {Segmentation} {Network}},
	volume = {3},
	issn = {2377-3766},
	doi = {10.1109/LRA.2018.2849498},
	abstract = {In fruit production, critical crop management decisions are guided by bloom intensity, i.e., the number of flowers present in an orchard. Despite its importance, bloom intensity is still typically estimated by means of human visual inspection. Existing automated computer vision systems for flower identification are based on hand-engineered techniques that work only under specific conditions and with limited performance. This letter proposes an automated technique for flower identification that is robust to uncontrolled environments and applicable to different flower species. Our method relies on an end-to-end residual convolutional neural network (CNN) that represents the state-of-the-art in semantic segmentation. To enhance its sensitivity to flowers, we fine-tune this network using a single dataset of apple flower images. Since CNNs tend to produce coarse segmentations, we employ a refinement method to better distinguish between individual flower instances. Without any preprocessing or dataset-specific training, experimental results on images of apple, peach, and pear flowers, acquired under different conditions demonstrate the robustness and broad applicability of our method.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Dias, P. A. and Tabb, A. and Medeiros, H.},
	month = oct,
	year = {2018},
	keywords = {Image color analysis, image segmentation, Image segmentation, Training, Agriculture, Bloom intensity estimation, agriculture, computer vision, feedforward neural nets, object detection, convolution, Image resolution, Semantics, flower detection, precision agriculture, semantic segmentation networks, Task analysis, apple flower images, automated computer vision systems, bloom intensity, CNN, crop management decisions, crops, end-to-end residual convolutional neural network, flower identification, fruit production, hand-engineered techniques, human visual inspection, multispecies fruit flower detection, orchard, peach flower images, pear flower images, refined semantic segmentation network},
	pages = {3003--3010},
	file = {IEEE Xplore Abstract Record:/home/atabb/Zotero/storage/BTNH4492/8392727.html:text/html}
}

@misc{noauthor_precision_nodate,
	title = {Precision {Agricultural} {Robotics} and {Autonomous} {Farming} {Technologies} - {IEEE} {Robotics} and {Automation} {Society}},
	url = {https://www.ieee-ras.org/publications/ra-l/special-issues/past-special-issues/precision-agricultural-robotics-and-autonomous-farming-technologies},
	abstract = {Focus is on both applied and theoretical issues in robotics and automation. Robotics is here defined to include intelligent machines and systems; whereas automation includes the use of automated methods in various applications to improve performance and productivity. The society sponsors a number of conferences, including the annual International Conference on Robotics and Automation.},
	language = {en-gb},
	urldate = {2019-03-21},
	file = {Snapshot:/home/atabb/Zotero/storage/5XTUEQQ7/precision-agricultural-robotics-and-autonomous-farming-technologies.html:text/html}
}

@incollection{siciliano_robotics_2016,
	address = {Cham},
	title = {Robotics in {Agriculture} and {Forestry}},
	isbn = {978-3-319-32550-7 978-3-319-32552-1},
	url = {http://link.springer.com/10.1007/978-3-319-32552-1_56},
	language = {en},
	urldate = {2019-03-21},
	booktitle = {Springer {Handbook} of {Robotics}},
	publisher = {Springer International Publishing},
	author = {Bergerman, Marcel and Billingsley, John and Reid, John and van Henten, Eldert},
	editor = {Siciliano, Bruno and Khatib, Oussama},
	year = {2016},
	doi = {10.1007/978-3-319-32552-1_56},
	pages = {1463--1492}
}

@book{billingsley_robotics_2019,
	address = {UK},
	title = {Robotics and automation for improving agriculture.},
	publisher = {Burleigh Dodds Science Publishing},
	editor = {Billingsley, John},
	year = {2019}
}

@article{ball_jfr_2017,
	title = {{JFR} {Special} {Issue} on {Agricultural} {Robotics}},
	volume = {34},
	copyright = {© 2017 Wiley Periodicals, Inc.},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21745},
	doi = {10.1002/rob.21745},
	language = {en},
	number = {6},
	urldate = {2019-03-20},
	journal = {Journal of Field Robotics},
	author = {Ball, David and Upcroft, Ben and Henten, Eldert van and Hengel, Anton van den and Tokekar, Pratap and Das, Jnaneshwar},
	year = {2017},
	pages = {1037--1038},
	file = {Snapshot:/home/atabb/Zotero/storage/E579X6WG/rob.html:text/html}
}

@article{bechar_agricultural_2016,
	title = {Agricultural robots for field operations: {Concepts} and components},
	volume = {149},
	issn = {15375110},
	shorttitle = {Agricultural robots for field operations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1537511015301914},
	doi = {10.1016/j.biosystemseng.2016.06.014},
	language = {en},
	urldate = {2019-03-22},
	journal = {Biosystems Engineering},
	author = {Bechar, Avital and Vigneault, Clément},
	month = sep,
	year = {2016},
	pages = {94--111}
}

@inproceedings{noauthor_autonomous_2019,
	title = {Autonomous {Technologies} in {Agricultural} {Equipment}: {A} {Review} of the {State} of the {Art}},
	shorttitle = {Autonomous {Technologies} in {Agricultural} {Equipment}},
	url = {https://elibrary.asabe.org/azdez.asp?JID=6&AID=49978&CID=913c0119&T=1},
	doi = {10.13031/913},
	abstract = {Automation and robotics for agricultural production are topics of tremendous interest and large investments. There have been significant recent advances in agricultural automation and robotics in the areas of (1) automatic vehicle guidance and steering control, (2) automatic implement guidance, (3) automatic headland sequence and turn management, (4) sensing for perception, (5) sensing for variable-rate technologies, (6) optimization of machine operation, (7) machinery coordination, and (8) machinery communication. These eight areas of progress are discussed in terms of commercially available technologies, public intellectual property, and the research literature.},
	language = {en},
	urldate = {2019-03-22},
	booktitle = {2019 {Agricultural} {Equipment} {Technology} {Conference}},
	publisher = {American Society of Agricultural and Biological Engineers},
	month = feb,
	year = {2019},
	pages = {1--17},
	file = {2019 - Autonomous Technologies in Agricultural Equipment.pdf:/home/atabb/Zotero/storage/ND7JKI65/2019 - Autonomous Technologies in Agricultural Equipment.pdf:application/pdf}
}

@inproceedings{noauthor_autonomous_2019-1,
	title = {Autonomous {Technologies} in {Agricultural} {Equipment}: {A} {Review} of the {State} of the {Art}},
	shorttitle = {Autonomous {Technologies} in {Agricultural} {Equipment}},
	url = {https://elibrary.asabe.org/azdez.asp?JID=6&AID=49978&CID=913c0119&T=1},
	doi = {10.13031/913},
	abstract = {Automation and robotics for agricultural production are topics of tremendous interest and large investments. There have been significant recent advances in agricultural automation and robotics in the areas of (1) automatic vehicle guidance and steering control, (2) automatic implement guidance, (3) automatic headland sequence and turn management, (4) sensing for perception, (5) sensing for variable-rate technologies, (6) optimization of machine operation, (7) machinery coordination, and (8) machinery communication. These eight areas of progress are discussed in terms of commercially available technologies, public intellectual property, and the research literature.},
	language = {en},
	urldate = {2019-03-22},
	booktitle = {2019 {Agricultural} {Equipment} {Technology} {Conference}},
	publisher = {American Society of Agricultural and Biological Engineers},
	month = feb,
	year = {2019},
	pages = {1--17},
	file = {2019 - Autonomous Technologies in Agricultural Equipment.pdf:/home/atabb/Zotero/storage/6M592VB6/2019 - Autonomous Technologies in Agricultural Equipment.pdf:application/pdf}
}

@article{asseng_future_2019,
	title = {Future farms without farmers},
	volume = {4},
	issn = {2470-9476},
	url = {http://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aaw1875},
	doi = {10.1126/scirobotics.aaw1875},
	language = {en},
	number = {27},
	urldate = {2019-03-22},
	journal = {Science Robotics},
	author = {Asseng, Senthold and Asche, Frank},
	month = feb,
	year = {2019},
	pages = {eaaw1875}
}

@article{lehnert_autonomous_2017,
	title = {Autonomous {Sweet} {Pepper} {Harvesting} for {Protected} {Cropping} {Systems}},
	volume = {2},
	issn = {2377-3766, 2377-3774},
	url = {http://ieeexplore.ieee.org/document/7827126/},
	doi = {10.1109/LRA.2017.2655622},
	number = {2},
	urldate = {2019-03-22},
	journal = {IEEE Robotics and Automation Letters},
	author = {Lehnert, Christopher and English, Andrew and McCool, Christopher and Tow, Adam W. and Perez, Tristan},
	month = apr,
	year = {2017},
	pages = {872--879},
	file = {Submitted Version:/home/atabb/Zotero/storage/Z46VYC4U/Lehnert et al. - 2017 - Autonomous Sweet Pepper Harvesting for Protected C.pdf:application/pdf}
}

@article{minervini_finely-grained_2016,
	title = {Finely-grained annotated datasets for image-based plant phenotyping},
	volume = {81},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865515003645},
	doi = {10.1016/j.patrec.2015.10.013},
	language = {en},
	urldate = {2019-03-22},
	journal = {Pattern Recognition Letters},
	author = {Minervini, Massimo and Fischbach, Andreas and Scharr, Hanno and Tsaftaris, Sotirios A.},
	month = oct,
	year = {2016},
	pages = {80--89},
	file = {Full Text:/home/atabb/Zotero/storage/KE2GGCU3/Minervini et al. - 2016 - Finely-grained annotated datasets for image-based .pdf:application/pdf}
}

@article{stein_image_2016,
	title = {Image {Based} {Mango} {Fruit} {Detection}, {Localisation} and {Yield} {Estimation} {Using} {Multiple} {View} {Geometry}},
	volume = {16},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/16/11/1915},
	doi = {10.3390/s16111915},
	language = {en},
	number = {11},
	urldate = {2019-03-22},
	journal = {Sensors},
	author = {Stein, Madeleine and Bargoti, Suchet and Underwood, James},
	month = nov,
	year = {2016},
	pages = {1915}
}

@article{bloch_methodology_2018,
	title = {A methodology of orchard architecture design for an optimal harvesting robot},
	volume = {166},
	issn = {15375110},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1537511017300612},
	doi = {10.1016/j.biosystemseng.2017.11.006},
	language = {en},
	urldate = {2019-03-22},
	journal = {Biosystems Engineering},
	author = {Bloch, Victor and Degani, Amir and Bechar, Avital},
	month = feb,
	year = {2018},
	pages = {126--137}
}

@misc{duckett_agricultural_2018,
	type = {Paper or {Report}},
	title = {Agricultural {Robotics}: {The} {Future} of {Robotic} {Agriculture}},
	copyright = {cc\_by4},
	shorttitle = {Agricultural {Robotics}},
	url = {http://hamlyn.doc.ic.ac.uk/uk-ras/sites/default/files/UK_RAS_wp_Agri_hi-res_single.pdf},
	abstract = {Agri-Food is the largest manufacturing sector in the UK. It supports a food chain that generates over £108bn p.a., with 3.9m employees in a truly international industry and exports £20bn of UK manufactured goods. However, the global food chain is under pressure from population growth, climate change, political pressures affecting migration, population drift from rural to urban regions and the demographics of an aging global population. These challenges are recognised in the UK Industrial Strategy white paper and backed by significant investment via a Wave 2 Industrial Challenge Fund Investment ("Transforming Food Production: from Farm to Fork"). Robotics and Autonomous Systems (RAS) and associated digital technologies are now seen as enablers of this critical food chain transformation. To meet these challenges, this white paper reviews the state of the art in the application of RAS in Agri-Food production and explores research and innovation needs to ensure these technologies reach their full potential and deliver the necessary impacts in the Agri-Food sector.},
	language = {en},
	urldate = {2019-03-22},
	author = {Duckett, Tom and Pearson, Simon and Blackmore, Simon and Grieve, Bruce and Chen, Wen-Hua and Cielniak, Grzegorz and Cleaversmith, Jason and Dai, Jian and Davis, Steve and Fox, Charles and From, Pal and Georgilas, Ioannis and Gill, Richie and Gould, Iain and Hanheide, Marc and Iida, Fumiya and Mihalyova, Lyudmila and Nefti-Meziani, Samia and Neumann, Gerhard and Paoletti, Paolo and Pridmore, Tony and Ross, Dave and Smith, Melvyn and Stoelen, Martin and Swainson, Mark and Wane, Sam and Wilson, Peter and Wright, Isobel and Yang, Guang-Zhong},
	month = jun,
	year = {2018}
}

@inproceedings{baillie_review_2018,
	title = {A review of the state of the art in agricultural automation. {Part} {I}: {Sensing} technologies for optimization of machine operation and farm inputs},
	shorttitle = {\&lt;i\&gt;{A} review of the state of the art in agricultural automation. {Part} {I}},
	url = {http://elibrary.asabe.org/abstract.asp?JID=5&AID=49328&CID=det2018&T=1},
	doi = {10.13031/aim.201801589},
	language = {en},
	urldate = {2019-03-22},
	booktitle = {2018 {Detroit}, {Michigan} {July} 29 - {August} 1, 2018},
	publisher = {American Society of Agricultural and Biological Engineers},
	author = {Baillie, Craig P. and Thomasson, J. Alex and Lobsey, Craig R. and McCarthy, Cheryl L. and Antille, Diogenes L.},
	year = {2018}
}

@article{williams_autonomous_2019,
	title = {Autonomous pollination of individual kiwifruit flowers: {Toward} a robotic kiwifruit pollinator},
	issn = {1556-4959, 1556-4967},
	shorttitle = {Autonomous pollination of individual kiwifruit flowers},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21861},
	doi = {10.1002/rob.21861},
	language = {en},
	urldate = {2019-03-22},
	journal = {Journal of Field Robotics},
	author = {Williams, Henry and Nejati, Mahla and Hussein, Salome and Penhall, Nicky and Lim, Jong Yoon and Jones, Mark Hedley and Bell, Jamie and Ahn, Ho Seok and Bradley, Stuart and Schaare, Peter and Martinsen, Paul and Alomar, Mohammad and Patel, Purak and Seabright, Matthew and Duke, Mike and Scarfe, Alistair and MacDonald, Bruce},
	month = jan,
	year = {2019}
}

@article{duckett_agricultural_2018-1,
	title = {Agricultural {Robotics}: {The} {Future} of {Robotic} {Agriculture}},
	shorttitle = {Agricultural {Robotics}},
	url = {https://arxiv.org/abs/1806.06762v2},
	abstract = {Agri-Food is the largest manufacturing sector in the UK. It supports a food
chain that generates over £108bn p.a., with 3.9m employees in a truly
international industry and exports £20bn of UK manufactured goods.
However, the global food chain is under pressure from population growth,
climate change, political pressures affecting migration, population drift from
rural to urban regions and the demographics of an aging global population.
These challenges are recognised in the UK Industrial Strategy white paper and
backed by significant investment via a Wave 2 Industrial Challenge Fund
Investment ("Transforming Food Production: from Farm to Fork"). Robotics and
Autonomous Systems (RAS) and associated digital technologies are now seen as
enablers of this critical food chain transformation. To meet these challenges,
this white paper reviews the state of the art in the application of RAS in
Agri-Food production and explores research and innovation needs to ensure these
technologies reach their full potential and deliver the necessary impacts in
the Agri-Food sector.},
	language = {en},
	urldate = {2019-03-22},
	author = {Duckett, Tom and Pearson, Simon and Blackmore, Simon and Grieve, Bruce and Chen, Wen-Hua and Cielniak, Grzegorz and Cleaversmith, Jason and Dai, Jian and Davis, Steve and Fox, Charles and From, Pål and Georgilas, Ioannis and Gill, Richie and Gould, Iain and Hanheide, Marc and Hunter, Alan and Iida, Fumiya and Mihalyova, Lyudmila and Nefti-Meziani, Samia and Neumann, Gerhard and Paoletti, Paolo and Pridmore, Tony and Ross, Dave and Smith, Melvyn and Stoelen, Martin and Swainson, Mark and Wane, Sam and Wilson, Peter and Wright, Isobel and Yang, Guang-Zhong},
	month = jun,
	year = {2018},
	file = {Full Text PDF:/home/atabb/Zotero/storage/TMIVJJ8E/Duckett et al. - 2018 - Agricultural Robotics The Future of Robotic Agric.pdf:application/pdf;Snapshot:/home/atabb/Zotero/storage/34GDLFQA/1806.html:text/html}
}

@article{pezzementi_comparing_2018,
	title = {Comparing apples and oranges: {Off}-road pedestrian detection on the {National} {Robotics} {Engineering} {Center} agricultural person-detection dataset},
	volume = {35},
	issn = {15564959},
	shorttitle = {Comparing apples and oranges},
	url = {http://doi.wiley.com/10.1002/rob.21760},
	doi = {10.1002/rob.21760},
	language = {en},
	number = {4},
	urldate = {2019-03-22},
	journal = {Journal of Field Robotics},
	author = {Pezzementi, Zachary and Tabor, Trenton and Hu, Peiyun and Chang, Jonathan K. and Ramanan, Deva and Wellington, Carl and Wisely Babu, Benzun P. and Herman, Herman},
	month = jun,
	year = {2018},
	pages = {545--563}
}

@inproceedings{kayacan_embedded_2018,
	title = {Embedded {High} {Precision} {Control} and {Corn} {Stand} {Counting} {Algorithms} for an {Ultra}-{Compact} 3D {Printed} {Field} {Robot}},
	isbn = {978-0-9923747-4-7},
	url = {http://www.roboticsproceedings.org/rss14/p36.pdf},
	doi = {10.15607/RSS.2018.XIV.036},
	abstract = {This paper presents embedded high precision control and corn stands counting algorithms for a low-cost, ultracompact 3D printed and autonomous ﬁeld robot for agricultural operations. Currently, plant traits, such as emergence rate, biomass, vigor and stand counting are measured manually. This is highly labor intensive and prone to errors. The robot, termed TerraSentia, is designed to automate the measurement of plant traits for efﬁcient phenotyping as an alternative to manual measurements. In this paper, we formulate a Nonlinear Moving Horizon Estimator (NMHE) that identiﬁes key terrain parameters using onboard robot sensors and a learning-based Nonlinear Model Predictive Control (NMPC) that ensures high precision path tracking in the presence of unknown wheel-terrain interaction. Moreover, we develop a machine vision algorithm to enable TerraSentia to count corn stands by driving through the ﬁelds autonomously. We present results of an extensive ﬁeld-test study that shows that (i) the robot can track paths precisely with less than 5 cm error so that the robot is less likely to damage plants, and (ii) the machine vision algorithm is robust against interferences from leaves and weeds, and the system has been veriﬁed in corn ﬁelds at the growth stage of V4, V6, VT, R2, and R6 from ﬁve different locations. The robot predictions agree well with the ground truth with countrobot = 0.96 × counthuman + 0.85 and correlation coefﬁcient R = 0.96.},
	language = {en},
	urldate = {2019-03-22},
	booktitle = {Robotics: {Science} and {Systems} {XIV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Kayacan, Erkan and Zhang, Zhongzhong and Chowdhary, Girish},
	month = jun,
	year = {2018},
	file = {Kayacan et al. - 2018 - Embedded High Precision Control and Corn Stand Cou.pdf:/home/atabb/Zotero/storage/3JRTXHKY/Kayacan et al. - 2018 - Embedded High Precision Control and Corn Stand Cou.pdf:application/pdf}
}

@book{karkee_fundamentals_2019,
	title = {Fundamentals of {Agricultural} and {Field} {Robotics}},
	publisher = {Springer},
	editor = {Karkee, Manoj and Zhang, Qin},
	year = {2019}
}

@article{barth_synthetic_2017,
	title = {Synthetic bootstrapping of convolutional neural networks for semantic plant part segmentation},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169917307664},
	doi = {10.1016/j.compag.2017.11.040},
	language = {en},
	urldate = {2019-03-22},
	journal = {Computers and Electronics in Agriculture},
	author = {Barth, R. and IJsselmuiden, J. and Hemming, J. and Van Henten, E.J.},
	month = dec,
	year = {2017}
}

@article{li_large_2016,
	title = {Large {Scale} {Image} {Mosaic} {Construction} for {Agricultural} {Applications}},
	volume = {1},
	issn = {2377-3766, 2377-3774},
	url = {http://ieeexplore.ieee.org/document/7387713/},
	doi = {10.1109/LRA.2016.2519946},
	number = {1},
	urldate = {2019-03-22},
	journal = {IEEE Robotics and Automation Letters},
	author = {Li, Zhengqi and Isler, Volkan},
	month = jan,
	year = {2016},
	pages = {295--302}
}

@article{kicherer_phenoliner:_2017,
	title = {Phenoliner: {A} {New} {Field} {Phenotyping} {Platform} for {Grapevine} {Research}},
	volume = {17},
	issn = {1424-8220},
	shorttitle = {Phenoliner},
	url = {http://www.mdpi.com/1424-8220/17/7/1625},
	doi = {10.3390/s17071625},
	language = {en},
	number = {7},
	urldate = {2019-03-22},
	journal = {Sensors},
	author = {Kicherer, Anna and Herzog, Katja and Bendel, Nele and Klück, Hans-Christian and Backhaus, Andreas and Wieland, Markus and Rose, Johann and Klingbeil, Lasse and Läbe, Thomas and Hohl, Christian and Petry, Willi and Kuhlmann, Heiner and Seiffert, Udo and Töpfer, Reinhard},
	month = jul,
	year = {2017},
	pages = {1625},
	file = {Full Text:/home/atabb/Zotero/storage/FQ3MJRXA/Kicherer et al. - 2017 - Phenoliner A New Field Phenotyping Platform for G.pdf:application/pdf}
}

@article{partel_development_2019,
	title = {Development and evaluation of a low-cost and smart technology for precision weed management utilizing artificial intelligence},
	volume = {157},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169918316612},
	doi = {10.1016/j.compag.2018.12.048},
	language = {en},
	urldate = {2019-03-22},
	journal = {Computers and Electronics in Agriculture},
	author = {Partel, Victor and Charan Kakarla, Sri and Ampatzidis, Yiannis},
	month = feb,
	year = {2019},
	pages = {339--350}
}

@article{cheein_guest_2017,
	title = {Guest {Editorial} {Focused} {Section} on {Mechatronics} {Applications} in {Agriculture}},
	volume = {22},
	issn = {1083-4435, 1941-014X},
	url = {http://ieeexplore.ieee.org/document/8197182/},
	doi = {10.1109/TMECH.2017.2774106},
	number = {6},
	urldate = {2019-03-22},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Cheein, F. A. and Kantor, G. and Reina, G. and Torres-Torriti, M.},
	month = dec,
	year = {2017},
	pages = {2397--2400}
}

@book{bechar_innovation_2019,
	title = {Innovation in {Agricultural} {Robotics} for {Precision} {Agriculture}},
	publisher = {Springer},
	editor = {Bechar, Avital},
	year = {2019}
}

@book{zhang_automation_2017,
	address = {Boston, MA},
	title = {Automation in tree fruit production: principles and practice},
	isbn = {978-1-78064-850-7},
	shorttitle = {Automation in tree fruit production},
	publisher = {CABI},
	editor = {Zhang, Qin},
	year = {2017},
	keywords = {Precision farming, Farm mechanization, Fruit trees},
	annote = {Tree fruit production automation -- The economics of perennial crops production automation -- Sensing for stress detection and high-throughput phenotyping in precision horticulture -- Light interception and canopy sensing for tree fruit canopy management -- Precision orchard systems -- Variable rate irrigation on center pivots -- Precision technologies for pest and disease management -- Precision nutrient management -- Precise crop load management -- Mechanical harvest and in-field handling of tree fruit crops -- Opportunity of robotics in precision horticulture}
}

@article{narvaez_survey_2017,
	title = {A {Survey} of {Ranging} and {Imaging} {Techniques} for {Precision} {Agriculture} {Phenotyping}},
	volume = {22},
	issn = {1083-4435},
	doi = {10.1109/TMECH.2017.2760866},
	abstract = {Agricultural production must double by 2050 in order to meet the expected food demand due to population growth. Precision agriculture is the key to improve productivity and efficiency in the use of resources, thus helping to achieve this goal under the diverse challenges currently faced by agriculture mainly due to climate changes, land degradation, availability of farmable land, labor force shortage, and increasing costs. To face these challenges, precision agriculture uses and develops sensing methodologies that provide information about crop growth and health indicators. This paper presents a survey of the state-of-the-art in optical visible and near-visible spectrum sensors and techniques to estimate phenotyping variables from intensity, spectral, and volumetric measurements. The sensing methodologies are classified into three areas according to the purpose of the measurements: 1) plant structural characterization; 2) plant/fruit detection; and 3) plant physiology assessment. This paper also discusses the progress in data processing methods and the current open challenges in agricultural tasks in which the development of innovative sensing methodologies is required, such as pruning, fertilizer and pesticide management, crop monitoring, and automated harvesting.},
	number = {6},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Narvaez, F. Yandun and Reina, G. and Torres-Torriti, M. and Kantor, G. and Cheein, F. A.},
	month = dec,
	year = {2017},
	keywords = {Agriculture, agriculture, Three-dimensional displays, Vegetation, productivity, Robot sensing systems, object detection, image sensors, phenotyping, precision agriculture, crops, precision engineering, Advanced sensing in agriculture, agricultural production, climate changes, crop growth, data processing, farmable land, food demand, Food production, fruit detection, health indicators, imaging techniques, labor force shortage, land degradation, Laser radar, morphology characterization, near-visible spectrum sensors, optical visible spectrum sensors, physiology assessment, plant physiology assessment, plant structural characterization, plant/fruit detection, population growth, Precision Agriculture Phenotyping, ranging techniques, sensing methodologies, Sensor phenomena and characterization},
	pages = {2428--2439},
	file = {IEEE Xplore Abstract Record:/home/atabb/Zotero/storage/QF4X4JF9/8062821.html:text/html}
}

@article{rosell-polo_kinect_2017,
	title = {Kinect v2 {Sensor}-{Based} {Mobile} {Terrestrial} {Laser} {Scanner} for {Agricultural} {Outdoor} {Applications}},
	volume = {22},
	issn = {1083-4435},
	doi = {10.1109/TMECH.2017.2663436},
	abstract = {Mobile terrestrial laser scanners (MTLS), based on light detection and ranging sensors, are used worldwide in agricultural applications. MTLS are applied to characterize the geometry and the structure of plants and crops for technical and scientific purposes. Although MTLS exhibit outstanding performance, their high cost is still a drawback for most agricultural applications. This paper presents a low-cost alternative to MTLS based on the combination of a Kinect v2 depth sensor and a real time kinematic global navigation satellite system (GNSS) with extended color information capability. The theoretical foundations of this system are exposed along with some experimental results illustrating their performance and limitations. This study is focused on open-field agricultural applications, although most conclusions can also be extrapolated to similar outdoor uses. The developed Kinect-based MTLS system allows to select different acquisition frequencies and fields of view (FOV), from one to 512 vertical slices. The authors conclude that the better performance is obtained when a FOV of a single slice is used, but at the price of a very low measuring speed. With that particular configuration, plants, crops, and objects are reproduced accurately. Future efforts will be directed to increase the scanning efficiency by improving both the hardware and software components and to make it feasible using both partial and full FOV.},
	number = {6},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Rosell-Polo, J. R. and Gregorio, E. and Gené, J. and Llorens, J. and Torrent, X. and Arnó, J. and Escolà, A.},
	month = dec,
	year = {2017},
	keywords = {precision horticulture, Agriculture, Three-dimensional displays, image sensors, image colour analysis, precision agriculture, crops, Food production, Laser radar, acquisition frequencies, agricultural outdoor applications, Depth cameras, extended color information capability, field-of-view, FOV, Frequency measurement, kinect v2, Kinect v2 depth sensor, light detection, light detection and ranging (LiDAR), Mobile communication, mobile terrestrial laser scanner, mobile terrestrial laser scanner (MTLS), MTLS system, open-field agricultural applications, optical scanners, precision fruticulture, ranging sensors, real-time kinematic global navigation satellite system, RGB-D cameras, satellite navigation, scientific purposes, Sensor systems, technical purposes, Time measurement},
	pages = {2420--2427},
	file = {Accepted Version:/home/atabb/Zotero/storage/CW4D3UN5/Rosell-Polo et al. - 2017 - Kinect v2 Sensor-Based Mobile Terrestrial Laser Sc.pdf:application/pdf;IEEE Xplore Abstract Record:/home/atabb/Zotero/storage/2D3AWCGF/7839931.html:text/html}
}

@article{siebald_real-time_2017,
	title = {Real-{Time} {Acoustic} {Monitoring} of {Cutting} {Blade} {Sharpness} in {Agricultural} {Machinery}},
	volume = {22},
	issn = {1083-4435},
	doi = {10.1109/TMECH.2017.2735542},
	abstract = {Cutting processes are among the most important crushing procedures in harvesting technology. Common practice attempts to identify the time for regrinding crop cutting blades do not often lead to the desired results. Continuous monitoring of the cutter bars in harvesting machines could provide the optimal regrinding time to maintain cutting performance and to achieve preferably long maintenance intervals. In this study, a method for real-time acoustic monitoring of the sharpness of crop cutting blades is shown based on an analytical simulation, acoustic measurements, and statistical analysis. These measurements were performed with piezoelectric accelerometers and signals were recorded at a sampling rate of 51 kHz. Structure-borne sound was measured on the counter blade, knife drum, and cabin of a self-propelled field chopper during harvesting. A good interrelation was found between the condition of the blades and the structure-borne sound. The statistical classification analysis with the support vector machine method allows an attribution of the blade sharpness (described by means of executed grinding cycles) with an accuracy of 0.76. Further development steps and the optimization potential of the design of system components are also discussed.},
	number = {6},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Siebald, H. and Hensel, O. and Beneke, F. and Merbach, L. and Walther, C. and Kirchner, S. M. and Huster, J.},
	month = dec,
	year = {2017},
	keywords = {Agriculture, agricultural machinery, Robot sensing systems, crops, Monitoring, Food production, Acoustic measurements, Acoustics, blades, condition monitoring, continuous monitoring, crop cutting blades, crushing procedures, cutting, cutting blade sharpness, cutting tools, forage chopper, grinding, harvesting machines, harvesting technology, real-time acoustic monitoring, real-time detection, statistical analysis, statistical classification analysis, support vector machine (SVM), support vector machine method, support vector machines},
	pages = {2411--2419},
	file = {IEEE Xplore Abstract Record:/home/atabb/Zotero/storage/FLVC9UZY/8000593.html:text/html}
}

@article{leu_robotic_2017,
	title = {Robotic {Green} {Asparagus} {Selective} {Harvesting}},
	volume = {22},
	issn = {1083-4435},
	doi = {10.1109/TMECH.2017.2735861},
	abstract = {Robotic harvesting in the open field demands innovative solutions in robot perception and mechanics to cope with environmental challenges. In this paper, a prototype robotic harvester that is able to cope with the challenges of selective harvesting of green asparagus is presented. The harvester is able to drive along an asparagus dam in the field, detect asparagus stalks, identify stalks that are ready for harvesting, and perform harvesting without damaging. These system abilities are enabled by a novel vision perception module and a novel harvesting mechanism. The perception module is based on three-dimensional-point cloud processing, and it is able to reliably and robustly detect the asparagus stalks, their positions, and dimensions. A novelty of the proposed harvesting mechanism is a multi-tools solution to increase harvesting productivity. The results of the first outdoor field tests demonstrate the applicability of the presented robotic harvester.},
	number = {6},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Leu, A. and Razavi, M. and Langstädtler, L. and Ristić-Durrant, D. and Raffel, H. and Schenck, C. and Gräser, A. and Kuhfuss, B.},
	month = dec,
	year = {2017},
	keywords = {agricultural products, mobile robots, robot vision, Agriculture, agriculture, industrial robots, Robot sensing systems, Robot kinematics, object detection, Agricultural robotics, Food production, asparagus dam, asparagus stalks detection, environmental challenges, green asparagus selective harvesting, Green products, harvesting productivity, Mechatronics, novel harvesting mechanism, open field, presented robotic harvester, Prototypes, robot mechanics, robot perception, robotic green asparagus selective harvesting, robotic harvester, robotic harvesting, robotic harvesting system, three-dimensional-point cloud processing, vision perception module},
	pages = {2401--2410},
	file = {IEEE Xplore Abstract Record:/home/atabb/Zotero/storage/XK3KD4L4/8031020.html:text/html;IEEE Xplore Full Text PDF:/home/atabb/Zotero/storage/ICL45RHP/Leu et al. - 2017 - Robotic Green Asparagus Selective Harvesting.pdf:application/pdf}
}

@book{reis_robot_2016,
	address = {Cham Heidelberg New York},
	series = {Advances in intelligent systems and computing},
	title = {{ROBOT} 2015: second {Iberian} {Robotics} {Conference}: advances in robotics. {Volume} 2: ...},
	isbn = {978-3-319-27149-1 978-3-319-27148-4},
	shorttitle = {{ROBOT} 2015},
	language = {eng},
	number = {418},
	publisher = {Springer},
	editor = {Reis, Luís Paulo and Moreira, António Paulo and Lima, Pedro U. and Montano, Luis and Muñoz-Martinez, Victor},
	year = {2016},
	note = {OCLC: 945769672},
	annote = {Literaturangaben},
	file = {Table of Contents PDF:/home/atabb/Zotero/storage/AFYL7KBS/Reis et al. - 2016 - ROBOT 2015 second Iberian Robotics Conference ad.pdf:application/pdf}
}

@article{young_beyond_2018,
	title = {Beyond {Precision} {Weed} {Control}: {A} {Model} for {True} {Integration}},
	volume = {32},
	issn = {1550-2740},
	shorttitle = {Beyond {Precision} {Weed} {Control}},
	url = {https://www.cambridge.org/core/product/identifier/S0890037X17000707/type/journal_article},
	doi = {10.1017/wet.2017.70},
	language = {en},
	number = {01},
	urldate = {2019-03-22},
	journal = {Weed Technology},
	author = {Young, Stephen L.},
	month = feb,
	year = {2018},
	pages = {7--10},
	file = {Submitted Version:/home/atabb/Zotero/storage/QQIKTDYB/Young - 2018 - Beyond Precision Weed Control A Model for True In.pdf:application/pdf}
}

@inproceedings{deremetz_path_2017,
	address = {Paris},
	title = {Path tracking of a four-wheel steering mobile robot: {A} robust off-road parallel steering strategy},
	isbn = {978-1-5386-1096-1},
	shorttitle = {Path tracking of a four-wheel steering mobile robot},
	url = {http://ieeexplore.ieee.org/document/8098670/},
	doi = {10.1109/ECMR.2017.8098670},
	urldate = {2019-03-28},
	booktitle = {2017 {European} {Conference} on {Mobile} {Robots} ({ECMR})},
	publisher = {IEEE},
	author = {Deremetz, Mathieu and Lenain, Roland and Couvent, Adrian and Cariou, Christophe and Thuilot, Benoit},
	month = sep,
	year = {2017},
	pages = {1--7}
}